{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6337cc89",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "All the dependencies for this notebook are included in the `requirements.txt` file included in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3752bc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# Importing the needed libraries\n",
    "import getpass\n",
    "import glob\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from radiant_mlhub import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet\n",
    "\n",
    "from pathlib import Path\n",
    "from random import choice\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import List, Any, Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52952d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models\n",
      "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.7/50.7 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting image-classifiers==1.0.0\n",
      "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting efficientnet==1.0.0\n",
      "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\dell\\anaconda3\\lib\\site-packages (from efficientnet==1.0.0->segmentation_models) (0.19.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.21.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (9.2.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.9.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.19.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from packaging>=20.0->scikit-image->efficientnet==1.0.0->segmentation_models) (3.0.9)\n",
      "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation_models\n",
      "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation_models-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d838a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\") #activate GPU resource for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc382f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'nasa_rwanda_field_boundary_competition'\n",
    "assets = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa90238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLHub API Key: ········\n"
     ]
    }
   ],
   "source": [
    "#Append your MLHUB_API_KEY after this cell is executed to download dataset\n",
    "os.environ['MLHUB_API_KEY'] =  getpass.getpass(prompt=\"MLHub API Key: \")\n",
    "dataset = Dataset.fetch(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa7a8a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nasa_rwanda_field_boundary_competition: fetch stac catalog: 160KB [00:00, 544.81KB/s]                                  \n",
      "INFO:radiant_mlhub.client.catalog_downloader:unarchive nasa_rwanda_field_boundary_competition.tar.gz ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Documents\\\\zindi\\\\nasa field boundary\\\\Nasa_harvest_field_boundary_competition\\\\nasa_rwanda_field_boundary_competition\\\\nasa_rwanda_field_boundary_competition_labels_train\\\\nasa_rwanda_field_boundary_competition_labels_train_09\\\\nasa_rwanda_field_boundary_competition_labels_train_09.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13912\\3689596573.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\radiant_mlhub\\models\\dataset.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, output_dir, asset_output_dir, catalog_only, if_exists, api_key, profile, bbox, intersects, datetime, collection_filter)\u001b[0m\n\u001b[0;32m    375\u001b[0m         )\n\u001b[0;32m    376\u001b[0m         \u001b[0mdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatalogDownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\radiant_mlhub\\client\\catalog_downloader.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;31m# call each step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;31m# inspect the error report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\radiant_mlhub\\client\\catalog_downloader.py\u001b[0m in \u001b[0;36m_unarchive_catalog_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatalog_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r:gz'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDownloadIfExistsOpts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m                 \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mmembers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmembers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[1;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                 \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0o700\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m             \u001b[1;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2045\u001b[1;33m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0m\u001b[0;32m   2046\u001b[0m                          numeric_owner=numeric_owner)\n\u001b[0;32m   2047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2085\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2086\u001b[1;33m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[0;32m   2087\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2088\u001b[0m                                  numeric_owner=numeric_owner)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2160\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[1;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[0;32m   2198\u001b[0m         \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2199\u001b[0m         \u001b[0mbufsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopybufsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2200\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mbltn_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2201\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Documents\\\\zindi\\\\nasa field boundary\\\\Nasa_harvest_field_boundary_competition\\\\nasa_rwanda_field_boundary_competition\\\\nasa_rwanda_field_boundary_competition_labels_train\\\\nasa_rwanda_field_boundary_competition_labels_train_09\\\\nasa_rwanda_field_boundary_competition_labels_train_09.json'"
     ]
    }
   ],
   "source": [
    "dataset.download(if_exists='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image snapshot dimensions\n",
    "IMG_WIDTH = 256 \n",
    "IMG_HEIGHT = 256 \n",
    "IMG_CHANNELS = 4 #we have the rgba bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ddebe",
   "metadata": {},
   "source": [
    "We have two sets of data: the train and test dataset, each having a list of file ids belonging to them.\n",
    "For model development purposes, we will use the training set(`train_tiles`) and use the test set for model prediction/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_items = f\"{dataset_id}/{dataset_id}_source_train\"\n",
    "train_label_items = f\"{dataset_id}/{dataset_id}_labels_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3962103",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13912\\1008093902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_source_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(os.walk(train_source_items))[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    extract the tile id and timestamp from a source image folder\n",
    "    e.g extract 'ID_YYYY_MM' from 'nasa_rwanda_field_boundary_competition_source_train_ID_YYYY_MM'\n",
    "    \"\"\"\n",
    "    s = s.replace(f\"{dataset_id}_source_\", '').split('_')[1:]\n",
    "    return '_'.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dcde75e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13912\\117986376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_tiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_source_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_tiles = [clean_string(s) for s in next(os.walk(train_source_items))[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbd418",
   "metadata": {},
   "source": [
    "Our source images have pixel values > 255, hence we need to apply normalisation on our images to generate a normalised image. We apply the min-max normalisation for this which is simply: $${\\text{all pixel values - minimum pixel value} \\over \\text{maximum pixel value - minimum pixel value}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e534f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(\n",
    "    array: np.ndarray\n",
    "):\n",
    "    \"\"\" normalise image to give a meaningful output \"\"\"\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return (array - array_min) / (array_max - array_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e56a84",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2060bd",
   "metadata": {},
   "source": [
    "In this notebook, we will perform data augmentation on our normalised images. This will be used to populate the model with data to obtain even more accurate results.\n",
    "\n",
    "We will employ the following data augmentation techniques on the dataset:\n",
    "- rotation, flipping, blurring.\n",
    "\n",
    "These techniques were thanks to the radix-ai GitHub repository, which can be accessed [here](https://github.com/radix-ai/agoro-field-boundary-detector).\n",
    "We will observe the results on a random source image and its associated label below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the 4 bands of the image\n",
    "tile = random.choice(train_tiles)\n",
    "print(tile)\n",
    "bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "bd1_array = bd1.read(1)\n",
    "bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "bd2_array = bd2.read(1)\n",
    "bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "bd3_array = bd3.read(1)\n",
    "bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "bd4_array = bd4.read(1)\n",
    "b01_norm = normalize(bd1_array)\n",
    "b02_norm = normalize(bd2_array)\n",
    "b03_norm = normalize(bd3_array)\n",
    "b04_norm = normalize(bd4_array)\n",
    "\n",
    "field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile.split('_')[0]}/raster_labels.tif\").read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206812fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/radix-ai/agoro-field-boundary-detector/tree/master/src/agoro_field_boundary_detector\n",
    "def t_linear(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    _: int = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply a linear (i.e. no) transformation and save.\"\"\"\n",
    "    return field, mask\n",
    "\n",
    "def t_rotation(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    rot: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Rotate the data.\"\"\"\n",
    "    assert rot in range(0, 3 + 1)\n",
    "    for _ in range(rot):\n",
    "        field = np.rot90(field)\n",
    "        mask = np.rot90(mask)\n",
    "    return field, mask\n",
    "\n",
    "def t_flip(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    idx: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Flip the data.\"\"\"\n",
    "    assert idx in range(0, 2 + 1)\n",
    "    if idx == 0:  # Diagonal\n",
    "        field = np.rot90(np.fliplr(field))\n",
    "        mask = np.rot90(np.fliplr(mask))\n",
    "    if idx == 1:  # Horizontal\n",
    "        field = np.flip(field, axis=0)\n",
    "        mask = np.flip(mask, axis=0)\n",
    "    if idx == 2:  # Vertical\n",
    "        field = np.flip(field, axis=1)\n",
    "        mask = np.flip(mask, axis=1)\n",
    "    return field, mask\n",
    "\n",
    "def t_blur(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    sigma: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Blur the image by applying a Gaussian filter.\"\"\"\n",
    "    assert 0 <= sigma <= 10\n",
    "    sigma_f = 1.0 + (sigma / 10)\n",
    "    field = np.copy(field)\n",
    "    for i in range(3):\n",
    "        field[:, :, i] = gaussian_filter(field[:, :, i], sigma=sigma_f)\n",
    "    return field, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(field:np.ndarray, mask:np.ndarray): \n",
    "    \"\"\"Show the field and corresponding mask.\"\"\"\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax1 = fig.add_subplot(121)  # left side\n",
    "    ax2 = fig.add_subplot(122)  # right side\n",
    "    ax1.imshow(field[:,:,0:3])  # rgb band\n",
    "    plt.gray()\n",
    "    ax2.imshow(mask)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(field, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f36ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_rotation(field, mask, rot=1) #rotation\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdefc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_flip(field, mask, idx=0) #flipping\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f46a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_blur(field, mask, sigma=5) #blur\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    write_folder: Path,\n",
    "    prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate data augmentations of the provided field and corresponding mask which includes:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "    :param field: Input array of the field to augment\n",
    "    :param mask: Input array of the corresponding mask to augment\n",
    "    :param write_folder: Folder (path) to write the results (augmentations) to\n",
    "    :param prefix: Field-specific prefix used when writing the augmentation results\n",
    "    \"\"\"\n",
    "    # Generate transformations\n",
    "    f, m = [0,1,2,3], [0,1,2,3] #dummy data. will be replaced\n",
    "    f[0],m[0] = t_linear(field, mask) #no augmentation\n",
    "    f[1],m[1] = t_rotation(field, mask, rot=1) #rotation\n",
    "    f[2],m[2] = t_flip(field, mask, idx=0) #flipping\n",
    "    f[3],m[3] = t_blur(field, mask, sigma=5) #blur\n",
    "    for i in range(len(f)):        \n",
    "        with open(write_folder +'/'+ f\"fields/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(f[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(write_folder +'/'+ f\"masks/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(m[i], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c76a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    field: List[np.ndarray],\n",
    "    mask: List[np.ndarray],\n",
    "    prefix: List[str],\n",
    "    write_folder: Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save data augmentations for all the fields and corresponding masks with the following:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "     - Gamma filter (brightness)\n",
    "    :param fields: Fields to augment\n",
    "    :param masks: Corresponding masks to augment\n",
    "    :param prefixes: Field-specific prefixes corresponding each field\n",
    "    :param write_folder: Path to write the results (augmentations) to\n",
    "    \"\"\"\n",
    "    generate(\n",
    "        field=field,\n",
    "        mask=mask,\n",
    "        prefix=prefix,\n",
    "        write_folder=write_folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply augmentation effects to training set\n",
    "for tile in train_tiles:\n",
    "    bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "    bd1_array = bd1.read(1)\n",
    "    bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "    bd2_array = bd2.read(1)\n",
    "    bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "    bd3_array = bd3.read(1)\n",
    "    bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "    bd4_array = bd4.read(1)\n",
    "    b01_norm = normalize(bd1_array)\n",
    "    b02_norm = normalize(bd2_array)\n",
    "    b03_norm = normalize(bd3_array)\n",
    "    b04_norm = normalize(bd4_array)\n",
    "\n",
    "    ids_list  = tile.split('_') # XX_YYYY_MM where XX is the training file id and YYYY_MM is the timestamp\n",
    "    tile_id   = ids_list[0]\n",
    "    timestamp = f\"{ids_list[1]}_{ids_list[2]}\"\n",
    "\n",
    "    field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "    mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile_id}/raster_labels.tif\").read(1) \n",
    "\n",
    "    #create a folder for the augmented images\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/fields\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/fields\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/masks\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/masks\")\n",
    "\n",
    "    main( #applying augmentation effects\n",
    "        field  = field,\n",
    "        mask   = mask,\n",
    "        prefix = tile_id,\n",
    "        write_folder = f\"./augmented_data/{timestamp}\"\n",
    "    ) #approximately 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = next(os.walk(f\"./augmented_data\"))[1] #Get all timestamps\n",
    "augmented_files = next(os.walk(f\"./augmented_data/{timestamps[0]}/fields\"))[2] #Get all augmented tile ids. can just use one timestamp\n",
    "X = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32) #time-series image\n",
    "y = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8) #mask for each scene\n",
    "i = 0\n",
    "for file in augmented_files:\n",
    "    idx = 0\n",
    "    augmented_id = file.split('.pkl')[0] #id without .pkl extension\n",
    "    temporal_fields = []\n",
    "    for timestamp in timestamps:\n",
    "        with open(f\"./augmented_data/{timestamp}/fields/{augmented_id}.pkl\", 'rb') as field:\n",
    "            field = pickle.load(field) \n",
    "        X[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx += IMG_CHANNELS\n",
    "    with open(f\"./augmented_data/{timestamp}/masks/{augmented_id}.pkl\", 'rb') as mask:\n",
    "        mask = pickle.load(mask)\n",
    "    y[i] = mask.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88dd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(0, len(augmented_files)) #sanity check\n",
    "random_image = random.randint(0, len(augmented_files)-1)\n",
    "show_image(X[random_image][:,:,0:3], y[random_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf9520",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We decided to use U-Net as it has shown impressive results over multiple domains in image segmentation.\n",
    "We will employ a ResNet34 backbone with our spatio-temporal U-Net model.\n",
    "\n",
    "This uses our 24 channels (6 timestamps * 4 bands per timestamp) and generates the predicted field boundary per scene.\n",
    "\n",
    "We will also use an 80:20 train:validation set split for model training.\n",
    "\n",
    "Since this is a binary segmentation problem (field boundary or no field boundary), we will use the `binary cross_entropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/sustainlab-group/ParcelDelineation/blob/master/models/unet.py\n",
    "def unet(pretrained_weights = None,input_size = (256,256,4)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_scheduler(epoch):\n",
    "    lr = 1e-4\n",
    "    '''\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    '''\n",
    "    print(\"Set Learning Rate : {}\".format(lr))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643843fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 24\n",
    "input_shape = (256,256,num_channels)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15275933",
   "metadata": {},
   "source": [
    "For the model, we will make use of two key metrics: **Recall** and **F1-score**.\n",
    "\n",
    "**Recall** evaluates how much of the field boundaries which were labelled were actually predicted as well while the **f1-score** combines the precision and recall by evaluating the harmonic mean.\n",
    "\n",
    "**NOTE** that the recall is the more important metric for this case as we are mostly concerned about the retrieved field boundaries out of the labelled field boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e689d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "def f1(y_true, y_pred):\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall_   = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall_)/(precision+recall_+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None \n",
    "model_unet = Unet(BACKBONE, encoder_weights='imagenet')\n",
    "new_model = keras.models.Sequential()\n",
    "new_model.add(Conv2D(3, (1,1), padding='same', activation='relu', input_shape=input_shape))\n",
    "new_model.add(model_unet)\n",
    "model = new_model \n",
    "#sm.metrics.FScore(beta=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate_scheduler(0)),\n",
    "              metrics=[recall,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e51a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train, y=y_train,\n",
    "              validation_data=(x_val, y_val),\n",
    "              steps_per_epoch = len(x_train)//batch_size,\n",
    "              validation_steps = len(x_val)//batch_size,\n",
    "              batch_size=batch_size, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./unet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"./unet_model.h5\", custom_objects ={\"recall\":sm.metrics.Recall(threshold=0.5), \"f1\": f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23753d38",
   "metadata": {},
   "source": [
    "### Loading test chips to run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_items = f\"{dataset_id}/{dataset_id}_source_test\"\n",
    "test_tiles = [clean_string(s) for s in next(os.walk(test_source_items))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a880ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_ids = set()\n",
    "for tile in test_tiles:\n",
    "    test_tile_ids.add(tile.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29312c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.empty((len(test_tile_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32)\n",
    "i = 0\n",
    "loaded_tiles = []\n",
    "for tile_id in test_tile_ids:\n",
    "    idx = 0\n",
    "    for timestamp in timestamps:\n",
    "        bd1 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B01.tif\")\n",
    "        bd1_array = bd1.read(1)\n",
    "        bd2 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B02.tif\")\n",
    "        bd2_array = bd2.read(1)\n",
    "        bd3 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B03.tif\")\n",
    "        bd3_array = bd3.read(1)\n",
    "        bd4 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B04.tif\")\n",
    "        bd4_array = bd4.read(1)\n",
    "        b01_norm = normalize(bd1_array)\n",
    "        b02_norm = normalize(bd2_array)\n",
    "        b03_norm = normalize(bd3_array)\n",
    "        b04_norm = normalize(bd4_array)\n",
    "        \n",
    "        field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "        X_test[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx+=IMG_CHANNELS\n",
    "    loaded_tiles.append(str(tile_id).zfill(2)) #track order test tiles are loaded into X to make sure tile id matches \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dictionary = {}\n",
    "for i in range(len(test_tile_ids)):\n",
    "    model_pred = model.predict(np.expand_dims(X_test[i], 0))\n",
    "    model_pred = model_pred[0]\n",
    "    model_pred = (model_pred >= 0.5).astype(np.uint8)\n",
    "    model_pred = model_pred.reshape(IMG_HEIGHT, IMG_WIDTH)\n",
    "    predictions_dictionary.update([(str(loaded_tiles[i]), pd.DataFrame(model_pred))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33186bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for key, value in predictions_dictionary.items():\n",
    "    ftd = value.unstack().reset_index().rename(columns={'level_0': 'row', 'level_1': 'column', 0: 'label'})\n",
    "    ftd['tile_row_column'] = f'Tile{key}_' + ftd['row'].astype(str) + '_' + ftd['column'].astype(str)\n",
    "    ftd = ftd[['tile_row_column', 'label']]\n",
    "    dfs.append(ftd)\n",
    "\n",
    "sub = pd.concat(dfs)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f\"./harvest_sample_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4d879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1456b81e0739e370391a1194b27bc839a9f5597c29befb4a579ec9fcd0f2acde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
